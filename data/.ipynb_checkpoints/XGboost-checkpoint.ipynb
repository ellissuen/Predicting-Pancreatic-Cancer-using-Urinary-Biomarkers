{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0b0bddd-3159-410d-b9de-0fadda40ef72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# open data \n",
    "\n",
    "file_path = 'data/Debernardi et al 2020 data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "data = df[['age', 'sex', 'diagnosis', 'stage', 'benign_sample_diagnosis', 'plasma_CA19_9', 'creatinine', 'LYVE1', 'REG1B', 'TFF1', 'REG1A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6e2835d7-9386-4c6f-b90d-6ab693d97340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(['stage', 'benign_sample_diagnosis'], axis =1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'sex' is a categorical column in your DataFrame\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, dtype=int)  # Instantiate the encoder\n",
    "encoded_sex = encoder.fit_transform(data[['sex']])  # Fit and transform the 'sex' column\n",
    "\n",
    "data['sex'] = encoded_sex\n",
    "data.rename(columns={'sex': 'is_male'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1ba0bfb8-28fe-4de3-9d14-d0e91d06e3ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>is_male</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>plasma_CA19_9</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>LYVE1</th>\n",
       "      <th>REG1B</th>\n",
       "      <th>TFF1</th>\n",
       "      <th>REG1A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1.83222</td>\n",
       "      <td>0.893219</td>\n",
       "      <td>52.948840</td>\n",
       "      <td>654.282174</td>\n",
       "      <td>1262.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97266</td>\n",
       "      <td>2.037585</td>\n",
       "      <td>94.467030</td>\n",
       "      <td>209.488250</td>\n",
       "      <td>228.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.78039</td>\n",
       "      <td>0.145589</td>\n",
       "      <td>102.366000</td>\n",
       "      <td>461.141000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.70122</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>60.579000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.21489</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>65.540000</td>\n",
       "      <td>41.088000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52026</td>\n",
       "      <td>7.058209</td>\n",
       "      <td>156.241000</td>\n",
       "      <td>525.178000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85956</td>\n",
       "      <td>8.341207</td>\n",
       "      <td>16.915000</td>\n",
       "      <td>245.947000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36851</td>\n",
       "      <td>7.674707</td>\n",
       "      <td>289.701000</td>\n",
       "      <td>537.286000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.33458</td>\n",
       "      <td>8.206777</td>\n",
       "      <td>205.930000</td>\n",
       "      <td>722.523000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>1.50423</td>\n",
       "      <td>8.200958</td>\n",
       "      <td>411.938275</td>\n",
       "      <td>2021.321078</td>\n",
       "      <td>13200.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  is_male  diagnosis  plasma_CA19_9  creatinine     LYVE1       REG1B  \\\n",
       "0     33        0          1           11.7     1.83222  0.893219   52.948840   \n",
       "1     81        0          1            NaN     0.97266  2.037585   94.467030   \n",
       "2     51        1          1            7.0     0.78039  0.145589  102.366000   \n",
       "3     61        1          1            8.0     0.70122  0.002805   60.579000   \n",
       "4     62        1          1            9.0     0.21489  0.000860   65.540000   \n",
       "..   ...      ...        ...            ...         ...       ...         ...   \n",
       "585   68        1          3            NaN     0.52026  7.058209  156.241000   \n",
       "586   71        0          3            NaN     0.85956  8.341207   16.915000   \n",
       "587   63        1          3            NaN     1.36851  7.674707  289.701000   \n",
       "588   75        0          3            NaN     1.33458  8.206777  205.930000   \n",
       "589   74        1          3         1488.0     1.50423  8.200958  411.938275   \n",
       "\n",
       "            TFF1      REG1A  \n",
       "0     654.282174   1262.000  \n",
       "1     209.488250    228.407  \n",
       "2     461.141000        NaN  \n",
       "3     142.950000        NaN  \n",
       "4      41.088000        NaN  \n",
       "..           ...        ...  \n",
       "585   525.178000        NaN  \n",
       "586   245.947000        NaN  \n",
       "587   537.286000        NaN  \n",
       "588   722.523000        NaN  \n",
       "589  2021.321078  13200.000  \n",
       "\n",
       "[590 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b600171b-907d-4d76-8888-c1c517552bf3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = data.drop(['diagnosis'], axis = 1)\n",
    "y = data[['diagnosis']] - 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df5c61db-624c-4cbb-9168-28fcb2a1357d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = xgb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f295e6eb-39a9-4e54-9716-27ae4eee424e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7542372881355932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d783a92c-c99b-4106-86a2-05cf9409e4d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[32  9  0]\n",
      " [10 22  7]\n",
      " [ 0  3 35]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3542eaf0-93c7-4627-918a-6ce2a9f3f681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.78      0.77        41\n",
      "           1       0.65      0.56      0.60        39\n",
      "           2       0.83      0.92      0.88        38\n",
      "\n",
      "    accuracy                           0.75       118\n",
      "   macro avg       0.75      0.76      0.75       118\n",
      "weighted avg       0.75      0.75      0.75       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c026d0-57bd-4390-ab29-c8149e7095ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddbf511-cce1-43fd-9e9f-25837f805701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying with HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7b299725-e8c5-47bb-a6c3-6138b174b670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HistGradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HistGradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>HistGradientBoostingClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "HistGradientBoostingClassifier()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "classifier = HistGradientBoostingClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0bf824a4-4032-4170-a48a-46ae4e3acd31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.652542372881356\n",
      "Confusion Matrix:\n",
      " [[27  5  9]\n",
      " [ 6 17 16]\n",
      " [ 1  4 33]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.66      0.72        41\n",
      "           1       0.65      0.44      0.52        39\n",
      "           2       0.57      0.87      0.69        38\n",
      "\n",
      "    accuracy                           0.65       118\n",
      "   macro avg       0.67      0.65      0.64       118\n",
      "weighted avg       0.68      0.65      0.64       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(\"Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661993a0-2633-4948-8e88-2dd5fe55511f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da370c9-93b0-4fff-a8f3-3bee86a5a392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6bb9a1-65f5-4c43-89e5-f62ddb96216e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9956cf8-fff1-4dc1-9e5e-5262544ef25a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b213c5-7479-44f8-915e-fe75c985e56f",
   "metadata": {},
   "source": [
    "# Now with the means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60f8901b-0a48-4b15-83cc-b139ff59b8cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Overwrite NaN values in 'plasma_CA19_9' with mean\n",
    "data['plasma_CA19_9'].fillna(data['plasma_CA19_9'].mean(), inplace=True)\n",
    "\n",
    "# Overwrite NaN values in 'REG1A' with mean\n",
    "data['REG1A'].fillna(data['REG1A'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ee806720-bee3-4736-9600-4d6be06283e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = data.drop(['diagnosis'], axis = 1)\n",
    "y = data[['diagnosis']] - 1\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7670019-6d80-4a01-839d-61cc5c864d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27d4463e-07f0-43b2-85c9-7f16ac091eec",
   "metadata": {},
   "source": [
    "# trying something else here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "359f7a22-cfec-41c9-851c-7be581d736d6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'LogisticRegression()' (type <class 'sklearn.linear_model._logistic.LogisticRegression'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 24\u001b[0m\n\u001b[0;32m     11\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m     12\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer_mean\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)),  \u001b[38;5;66;03m# Imputer with strategy='mean'\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimputer_median\u001b[39m\u001b[38;5;124m'\u001b[39m, SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmedian\u001b[39m\u001b[38;5;124m'\u001b[39m)),  \u001b[38;5;66;03m# Imputer with strategy='median'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgb_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m, GradientBoostingClassifier())\n\u001b[0;32m     21\u001b[0m ])\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Fit the pipeline on training data\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the pipeline\u001b[39;00m\n\u001b[0;32m     27\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mscore(X_test, y_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:416\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    415\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 416\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:350\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;66;03m# shallow copy of steps - this should really be steps_\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps)\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_steps()\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;66;03m# Setup the memory\u001b[39;00m\n\u001b[0;32m    352\u001b[0m     memory \u001b[38;5;241m=\u001b[39m check_memory(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:234\u001b[0m, in \u001b[0;36mPipeline._validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[0;32m    232\u001b[0m         t, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m     ):\n\u001b[1;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll intermediate steps should be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers and implement fit and transform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor be the string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    238\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (t, \u001b[38;5;28mtype\u001b[39m(t))\n\u001b[0;32m    239\u001b[0m         )\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# We allow last estimator to be None as an identity transformation\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     estimator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    246\u001b[0m ):\n",
      "\u001b[1;31mTypeError\u001b[0m: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' 'LogisticRegression()' (type <class 'sklearn.linear_model._logistic.LogisticRegression'>) doesn't"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "# Create a pipeline with two imputers and a classifier\n",
    "pipeline = Pipeline([\n",
    "    ('imputer_mean', SimpleImputer(strategy='mean')),  # Imputer with strategy='mean'\n",
    "    ('imputer_median', SimpleImputer(strategy='median')),  # Imputer with strategy='median'\n",
    "    ('standard_scaler', StandardScaler()),  # StandardScaler\n",
    "    ('min_max_scaler', MinMaxScaler()),\n",
    "    ('log_reg_classifier', LogisticRegression()),\n",
    "    ('forest_classifier', RandomForestClassifier()),\n",
    "    ('svc_classifier', SVC()),\n",
    "    ('knn_classifier', KNeighborsClassifier()),\n",
    "    ('gb_classifier', GradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the pipeline\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efa048a-a3c5-40f5-afe8-53616584c3a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248a774a-86a0-48a3-96ed-7a1ed3090f2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e508d28d-c54b-439c-b524-63e1241ec31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e151aa9-4c4a-4065-8a84-cace00e9db49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951e29f-0690-4f2d-bb6e-456adeb80d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa7ec1-61d2-407f-bffb-d94d459b0489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dd079-4b48-453a-b7ca-110c3b469d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c4247-e0b3-43cf-a77e-2bebd6c0a228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f09b5f97-5cf1-43a9-99de-8fc60b2c43fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# open data \n",
    "\n",
    "file_path = 'data/Debernardi et al 2020 data.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "data = df[['age', 'sex', 'diagnosis', 'stage', 'benign_sample_diagnosis', 'plasma_CA19_9', 'creatinine', 'LYVE1', 'REG1B', 'TFF1', 'REG1A']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9710878-242d-4280-8c86-722299848b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.drop(['stage', 'benign_sample_diagnosis'], axis =1)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Assuming 'sex' is a categorical column in your DataFrame\n",
    "\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, dtype=int)  # Instantiate the encoder\n",
    "encoded_sex = encoder.fit_transform(data[['sex']])  # Fit and transform the 'sex' column\n",
    "\n",
    "data['sex'] = encoded_sex\n",
    "data.rename(columns={'sex': 'is_male'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cd5a1187-cc98-4ec6-855c-dd1d0fe2c6ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Overwrite NaN values in 'plasma_CA19_9' with mean\n",
    "data['plasma_CA19_9'].fillna(data['plasma_CA19_9'].mean(), inplace=True)\n",
    "\n",
    "# Overwrite NaN values in 'REG1A' with mean\n",
    "data['REG1A'].fillna(data['REG1A'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f5b2795-0b79-4fe2-a539-2a9a83aaaf85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = data.drop(['diagnosis'], axis = 1)\n",
    "y = data[['diagnosis']] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c77cc834-13ae-4f66-af2a-0c134a1a4a47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model Accuracy: 0.711864406779661\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.72        41\n",
      "           1       0.58      0.54      0.56        39\n",
      "           2       0.81      0.89      0.85        38\n",
      "\n",
      "    accuracy                           0.71       118\n",
      "   macro avg       0.71      0.71      0.71       118\n",
      "weighted avg       0.71      0.71      0.71       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to numpy arrays\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "# Create a pipeline with imputers, scalers, and classifiers\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer()),  # Imputer with default strategy (mean)\n",
    "    ('scaler', StandardScaler()),  # StandardScaler\n",
    "    ('classifier', LogisticRegression())  # Default classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'imputer__strategy': ['mean', 'median'],  # Imputer strategies to try\n",
    "    'scaler': [StandardScaler(), MinMaxScaler()],  # Scalers to try\n",
    "    'classifier': [LogisticRegression(), RandomForestClassifier(), GradientBoostingClassifier(), SVC(), KNeighborsClassifier()]  # Classifiers to try\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Best Model Accuracy:\", accuracy)\n",
    "\n",
    "# View classification report for the best model\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "590549c0-78bc-4688-97b2-df6bc56cfeb8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All combinations that were tried:\n",
      "Mean accuracy: 0.604 - Parameters: {'classifier': LogisticRegression(), 'imputer__strategy': 'mean', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.551 - Parameters: {'classifier': LogisticRegression(), 'imputer__strategy': 'mean', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.631 - Parameters: {'classifier': LogisticRegression(), 'imputer__strategy': 'median', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.544 - Parameters: {'classifier': LogisticRegression(), 'imputer__strategy': 'median', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.663 - Parameters: {'classifier': RandomForestClassifier(), 'imputer__strategy': 'mean', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.652 - Parameters: {'classifier': RandomForestClassifier(), 'imputer__strategy': 'mean', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.686 - Parameters: {'classifier': RandomForestClassifier(), 'imputer__strategy': 'median', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.688 - Parameters: {'classifier': RandomForestClassifier(), 'imputer__strategy': 'median', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.678 - Parameters: {'classifier': GradientBoostingClassifier(), 'imputer__strategy': 'mean', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.674 - Parameters: {'classifier': GradientBoostingClassifier(), 'imputer__strategy': 'mean', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.680 - Parameters: {'classifier': GradientBoostingClassifier(), 'imputer__strategy': 'median', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.674 - Parameters: {'classifier': GradientBoostingClassifier(), 'imputer__strategy': 'median', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.568 - Parameters: {'classifier': SVC(), 'imputer__strategy': 'mean', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.544 - Parameters: {'classifier': SVC(), 'imputer__strategy': 'mean', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.597 - Parameters: {'classifier': SVC(), 'imputer__strategy': 'median', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.553 - Parameters: {'classifier': SVC(), 'imputer__strategy': 'median', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.525 - Parameters: {'classifier': KNeighborsClassifier(), 'imputer__strategy': 'mean', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.534 - Parameters: {'classifier': KNeighborsClassifier(), 'imputer__strategy': 'mean', 'scaler': MinMaxScaler()}\n",
      "Mean accuracy: 0.530 - Parameters: {'classifier': KNeighborsClassifier(), 'imputer__strategy': 'median', 'scaler': StandardScaler()}\n",
      "Mean accuracy: 0.544 - Parameters: {'classifier': KNeighborsClassifier(), 'imputer__strategy': 'median', 'scaler': MinMaxScaler()}\n",
      "\n",
      "Best Model Accuracy: 0.7542372881355932\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.78      0.75        41\n",
      "           1       0.70      0.54      0.61        39\n",
      "           2       0.82      0.95      0.88        38\n",
      "\n",
      "    accuracy                           0.75       118\n",
      "   macro avg       0.75      0.76      0.75       118\n",
      "weighted avg       0.75      0.75      0.75       118\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to numpy arrays\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "# Create a pipeline with imputers, scalers, and classifiers\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer()),  # Imputer with default strategy (mean)\n",
    "    ('scaler', StandardScaler()),  # StandardScaler\n",
    "    ('classifier', LogisticRegression())  # Default classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'imputer__strategy': ['mean', 'median'],  # Imputer strategies to try\n",
    "    'scaler': [StandardScaler(), MinMaxScaler()],  # Scalers to try\n",
    "    'classifier': [LogisticRegression(), RandomForestClassifier(), GradientBoostingClassifier(), SVC(), KNeighborsClassifier()]  # Classifiers to try\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print out all the combinations that were tried\n",
    "print(\"All combinations that were tried:\")\n",
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"Mean accuracy: {mean_score:.3f} - Parameters: {params}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nBest Model Accuracy:\", accuracy)\n",
    "\n",
    "# View classification report for the best model\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4c0f812b-452c-44fc-9295-860fbb94f8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1207, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 416, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Perform grid search with cross-validation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Print out all the combinations that were tried\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll combinations that were tried:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[0;32m    873\u001b[0m     )\n\u001b[1;32m--> 875\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_score)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[0;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    417\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 30 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1207, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nLogisticRegression does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 348, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n\n--------------------------------------------------------------------------------\n10 fits failed with the following error:\nTraceback (most recent call last):\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 420, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 416, in fit\n    X, y = self._validate_data(\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 621, in _validate_data\n    X, y = check_X_y(X, y, **check_params)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1147, in check_X_y\n    X = check_array(\n        ^^^^^^^^^^^^\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 959, in check_array\n    _assert_all_finite(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 124, in _assert_all_finite\n    _assert_all_finite_element_wise(\n  File \"C:\\Users\\ellis\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 173, in _assert_all_finite_element_wise\n    raise ValueError(msg_err)\nValueError: Input X contains NaN.\nGradientBoostingClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert y_train and y_test to numpy arrays\n",
    "y_train = np.ravel(y_train)\n",
    "y_test = np.ravel(y_test)\n",
    "\n",
    "# Create a pipeline with imputers, scalers, and classifiers\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # StandardScaler\n",
    "    ('classifier', LogisticRegression())  # Default classifier\n",
    "])\n",
    "\n",
    "# Define hyperparameters grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'scaler': [StandardScaler(), MinMaxScaler()],  # Scalers to try\n",
    "    'classifier': [LogisticRegression(), RandomForestClassifier(), GradientBoostingClassifier()]  # Classifiers to try\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print out all the combinations that were tried\n",
    "print(\"All combinations that were tried:\")\n",
    "results = grid_search.cv_results_\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"Mean accuracy: {mean_score:.3f} - Parameters: {params}\")\n",
    "\n",
    "# Evaluate the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nBest Model Accuracy:\", accuracy)\n",
    "\n",
    "# View classification report for the best model\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1e8a15c6-472e-40c9-96ef-a4cf6ea00087",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>patient_cohort</th>\n",
       "      <th>sample_origin</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>stage</th>\n",
       "      <th>benign_sample_diagnosis</th>\n",
       "      <th>plasma_CA19_9</th>\n",
       "      <th>creatinine</th>\n",
       "      <th>LYVE1</th>\n",
       "      <th>REG1B</th>\n",
       "      <th>TFF1</th>\n",
       "      <th>REG1A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1</td>\n",
       "      <td>Cohort1</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>33</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.7</td>\n",
       "      <td>1.83222</td>\n",
       "      <td>0.893219</td>\n",
       "      <td>52.948840</td>\n",
       "      <td>654.282174</td>\n",
       "      <td>1262.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S10</td>\n",
       "      <td>Cohort1</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>81</td>\n",
       "      <td>F</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97266</td>\n",
       "      <td>2.037585</td>\n",
       "      <td>94.467030</td>\n",
       "      <td>209.488250</td>\n",
       "      <td>228.407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S100</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>51</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.78039</td>\n",
       "      <td>0.145589</td>\n",
       "      <td>102.366000</td>\n",
       "      <td>461.141000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S101</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.70122</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>60.579000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S102</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>62</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.21489</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>65.540000</td>\n",
       "      <td>41.088000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>S549</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>68</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.52026</td>\n",
       "      <td>7.058209</td>\n",
       "      <td>156.241000</td>\n",
       "      <td>525.178000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>S558</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>71</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.85956</td>\n",
       "      <td>8.341207</td>\n",
       "      <td>16.915000</td>\n",
       "      <td>245.947000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>S560</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>63</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36851</td>\n",
       "      <td>7.674707</td>\n",
       "      <td>289.701000</td>\n",
       "      <td>537.286000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>S583</td>\n",
       "      <td>Cohort2</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>75</td>\n",
       "      <td>F</td>\n",
       "      <td>3</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.33458</td>\n",
       "      <td>8.206777</td>\n",
       "      <td>205.930000</td>\n",
       "      <td>722.523000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>S590</td>\n",
       "      <td>Cohort1</td>\n",
       "      <td>BPTB</td>\n",
       "      <td>74</td>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>IV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1488.0</td>\n",
       "      <td>1.50423</td>\n",
       "      <td>8.200958</td>\n",
       "      <td>411.938275</td>\n",
       "      <td>2021.321078</td>\n",
       "      <td>13200.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id patient_cohort sample_origin  age sex  diagnosis stage  \\\n",
       "0          S1        Cohort1          BPTB   33   F          1   NaN   \n",
       "1         S10        Cohort1          BPTB   81   F          1   NaN   \n",
       "2        S100        Cohort2          BPTB   51   M          1   NaN   \n",
       "3        S101        Cohort2          BPTB   61   M          1   NaN   \n",
       "4        S102        Cohort2          BPTB   62   M          1   NaN   \n",
       "..        ...            ...           ...  ...  ..        ...   ...   \n",
       "585      S549        Cohort2          BPTB   68   M          3    IV   \n",
       "586      S558        Cohort2          BPTB   71   F          3    IV   \n",
       "587      S560        Cohort2          BPTB   63   M          3    IV   \n",
       "588      S583        Cohort2          BPTB   75   F          3    IV   \n",
       "589      S590        Cohort1          BPTB   74   M          3    IV   \n",
       "\n",
       "    benign_sample_diagnosis  plasma_CA19_9  creatinine     LYVE1       REG1B  \\\n",
       "0                       NaN           11.7     1.83222  0.893219   52.948840   \n",
       "1                       NaN            NaN     0.97266  2.037585   94.467030   \n",
       "2                       NaN            7.0     0.78039  0.145589  102.366000   \n",
       "3                       NaN            8.0     0.70122  0.002805   60.579000   \n",
       "4                       NaN            9.0     0.21489  0.000860   65.540000   \n",
       "..                      ...            ...         ...       ...         ...   \n",
       "585                     NaN            NaN     0.52026  7.058209  156.241000   \n",
       "586                     NaN            NaN     0.85956  8.341207   16.915000   \n",
       "587                     NaN            NaN     1.36851  7.674707  289.701000   \n",
       "588                     NaN            NaN     1.33458  8.206777  205.930000   \n",
       "589                     NaN         1488.0     1.50423  8.200958  411.938275   \n",
       "\n",
       "            TFF1      REG1A  \n",
       "0     654.282174   1262.000  \n",
       "1     209.488250    228.407  \n",
       "2     461.141000        NaN  \n",
       "3     142.950000        NaN  \n",
       "4      41.088000        NaN  \n",
       "..           ...        ...  \n",
       "585   525.178000        NaN  \n",
       "586   245.947000        NaN  \n",
       "587   537.286000        NaN  \n",
       "588   722.523000        NaN  \n",
       "589  2021.321078  13200.000  \n",
       "\n",
       "[590 rows x 14 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df00893-fcc4-415d-a49d-cde7f771c16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
